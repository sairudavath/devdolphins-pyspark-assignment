# devdolphins-pyspark-assignment
# DevDolphins PySpark Data Engineer Assignment

**Candidate:** Rudavath  Sai
**Role:** PySpark Data Engineer  
**Submission Date:** 3 July 2025

---

## ğŸ“Œ Problem Statement
You are given:
- **`transactions.csv`**: Transaction data between customers and merchants
- **`CustomerImportance.csv`**: Transaction weights for different transaction types and customers

### Goal:
Design two real-time data processing mechanisms:

- **Mechanism X**:
  - Every second, read the next 10,000 transactions
  - Upload them to an S3 folder (`transactions_stream/`)

- **Mechanism Y**:
  - Monitor S3 for new transaction chunks
  - Detect specific patterns in transactions
  - Write 50 detections at a time to an S3 folder (`detections/`)
  - Track processed files using PostgreSQL

---

## ğŸ§± Architecture Diagram

```
DBFS (CSV) 
   â”‚
   â–¼
Mechanism X â”€â”€â–¶ S3 (transactions_stream/)
                         â”‚
                         â–¼
                   Mechanism Y
                         â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                                     â–¼
S3 (detections/)                    PostgreSQL (processed_files)
```

---

## ğŸ§ª Pattern Detection Rules

### âœ… Pattern 1: `PatId1 - UPGRADE`
- Top 10% customer by number of transactions
- Bottom 10% weight (avg of transaction types)
- Merchant must have â‰¥ 50,000 transactions
- **ActionType:** `UPGRADE`

### âœ… Pattern 2: `PatId2 - CHILD`
- Avg transaction value < â‚¹23
- At least 80 transactions with that merchant
- **ActionType:** `CHILD`

### âœ… Pattern 3: `PatId3 - DEI-NEEDED`
- More male than female customers overall
- But female customers > 100
- **ActionType:** `DEI-NEEDED`

---

## ğŸ—‚ Project Structure

```
devdolphins-pyspark-assignment/
â”œâ”€â”€ mechanism_x_upload_chunks.py     # Upload chunks to S3
â”œâ”€â”€ mechanism_y_detect_patterns.py   # Detect patterns and write to S3
â”œâ”€â”€ zip_detections_and_upload.py     # Compress and upload detections
â”œâ”€â”€ README.md
â”œâ”€â”€ sample_output/
â”‚   â””â”€â”€ detect_*.csv
```

---

## ğŸ“¦ Output Example (Detection File)
```csv
YStartTime,detectionTime,patternId,actionType,customerName,MerchantId
2025-07-03 16:01:52,2025-07-03 16:01:52,PatId2,CHILD,customer_12,merchant_8
2025-07-03 16:01:52,2025-07-03 16:01:52,PatId3,DEI-NEEDED,,merchant_6
```
- Each detection file contains **50 records max**
- Stored in `s3://<bucket>/detections/`

---



## ğŸ§‘â€ğŸ’» How to Run

### âš™ï¸ Step 1: Setup AWS & PostgreSQL Credentials
Export AWS keys as environment variables or set up `.aws/credentials` locally.

### ğŸƒ Step 2: Run Mechanism X
Uploads 10K transactions per second to S3.
```bash
python mechanism_x_upload_chunks.py
```

### ğŸ¤– Step 3: Run Mechanism Y
Continuously monitors S3 for chunks and writes detections.
```bash
python mechanism_y_detect_patterns.py
```

### ğŸ“¦ Step 4: Zip Detection Files
```bash
python zip_detections_and_upload.py
```

---



---

## âœ… Assumptions Made
- `customer` in transactions = `CustomerName`
- `merchant` = `MerchantId`
- `category` = `TransactionType`
- No fraud field was used as per instructions
- Data is clean and schema matches given CSVs

---

## ğŸ™Œ Thank You!
> Looking forward to contributing to your data engineering team ğŸš€
